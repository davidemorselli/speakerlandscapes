% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/train_embedding_model.R
\name{train_embedding_model}
\alias{train_embedding_model}
\title{Train a Word2Vec Embedding Model from Raw Text Data}
\usage{
train_embedding_model(
  file_path,
  output_file = "embeddings.emb",
  dim = 250,
  window = 10,
  min_count = 5,
  iter = 15,
  threads = 7,
  seed = 123
)
}
\arguments{
\item{file_path}{A character string specifying the path to the raw input text file. The file should contain speaker identifier as the first token of each line in. Speakers identifiers should start with a non equivocal token (e.g., speaker_ID1, speaker_ID2, ...), see \code{prepare_data()}}

\item{output_file}{A character string specifying the file path where the
trained Word2Vec model (\code{.emb} format) will be saved.
Defaults to \code{"embeddings.emb"}.}

\item{dim}{An integer specifying the dimensionality (vector size) of the
word embeddings. Defaults to \code{250}.}

\item{window}{An integer specifying the context window size for the skip-gram model.
Defaults to \code{10}.}

\item{min_count}{An integer specifying the minimum frequency required for a
token to be included in the vocabulary. Defaults to \code{5}.}

\item{iter}{An integer specifying the number of training iterations (epochs).
Defaults to \code{15}.}

\item{threads}{An integer specifying the number of CPU threads to use for
parallel computing during training. Defaults to \code{7}.}

\item{seed}{An integer used to set the random seed for reproducibility.
Defaults to \code{123}.}
}
\value{
The trained \code{word2vec} model object (an S3 object of class \code{word2vec}).
The model is also saved to disk at the path specified by \code{output_file}.
}
\description{
This function orchestrates the entire training pipeline for a skip-gram Word2Vec
model. It first preprocesses the raw text data (using the \code{preprocess_data}
function), trains the model, and finally saves the resulting
vector embeddings to a specified file path.
}
\details{
This function assumes the existence of two helper functions in the current R environment:
\enumerate{
\item \code{preprocess_data()}: Loads and tokenizes the raw data. It must return a
list of character vectors (tokenized sentences).
\item \code{get_most_frequent_tokens()}: Used to identify the top tokens (though not directly
called in the training flow, it is typically used for downstream analysis).
}
}
\examples{
\dontrun{
# Define placeholder helpers for a runnable example context
preprocess_data <- function(data, share_data, stopwords_language, custom_stopwords) {
  # Reads file and returns tokenized data
  list(c("this", "is", "a", "test", "sentence"), c("another", "test", "sentence", "here"))
}

# Example file path (replace with actual path)
dummy_file_path <- "corpus.txt"

# Train the model with default parameters, saving to "my_embeddings.emb"
trained_model <- train_embedding_model(
  file_path = dummy_file_path,
  output_file = "my_embeddings.emb",
  dim = 100,
  iter = 10
)

# You can then access the embeddings:
# embedding_matrix <- as.matrix(trained_model)
}
}
