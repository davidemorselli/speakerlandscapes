% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/get_most_frequent_tokens.R
\name{get_most_frequent_tokens}
\alias{get_most_frequent_tokens}
\title{Get Most Frequent Tokens from a Token List (with Stopword Removal using quanteda)}
\usage{
get_most_frequent_tokens(
  token_list,
  n_top = 100,
  min_count = 100,
  stopword_language = "en",
  custom_stopwords = NULL
)
}
\arguments{
\item{token_list}{A list of character vectors (tokenized documents).}

\item{n_top}{The number of top tokens to return. Defaults to 100.}

\item{min_count}{The minimum number of times a token must appear. Defaults to 100.}

\item{stopword_language}{The language for the stopwords. Defaults to "en".}

\item{custom_stopwords}{A character vector of additional stop words to remove.}
}
\value{
A tibble with columns 'token' and 'n' (count), sorted by frequency.
}
\description{
Takes a list of character vectors, removes stopwords,
and returns a tibble of the most frequent tokens and their counts.
Words to be removed can be standard stopwords (from stopwords/quanteda) or a custom list.
}
\examples{
\dontrun{
docs <- list(
  c("new", "york", "is", "a", "city", "city"),
  c("new", "york", "is", "a", "big", "city"),
  c("the", "city", "is", "great", "city")
)
# Note: For a real test, min_count should be low for this small example.
top_tokens <- get_most_frequent_tokens(docs, n_top = 5, min_count = 1)
print(top_tokens)
# Expected output (excluding "is", "a", "the", etc.):
# token     n
# <chr> <dbl>
# city      4
# new       2
# york      2
}
}
